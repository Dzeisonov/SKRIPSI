{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from glob import glob\n",
        "\n",
        "# --- Path dataset asli ---\n",
        "ORIG_DIR   = '/content/drive/MyDrive/Dataset Skripsi (15 Kelas Kran)'\n",
        "ALLOWED_EXT = ('.png', '.jpg', '.jpeg', '.webp', '.avif', '.jfif')\n",
        "\n",
        "# --- Hitung jumlah gambar per kelas ---\n",
        "class_counts = {}\n",
        "total_images = 0\n",
        "\n",
        "for class_name in sorted(os.listdir(ORIG_DIR)):\n",
        "    class_src = os.path.join(ORIG_DIR, class_name)\n",
        "    if not os.path.isdir(class_src):\n",
        "        continue\n",
        "\n",
        "    images = [p for p in glob(os.path.join(class_src, '*'))\n",
        "              if os.path.splitext(p)[1].lower() in ALLOWED_EXT]\n",
        "    n = len(images)\n",
        "    class_counts[class_name] = n\n",
        "    total_images += n\n",
        "\n",
        "# --- Tampilkan hasil ---\n",
        "print(\"Jumlah data per kelas (dataset awal):\")\n",
        "for cls, count in class_counts.items():\n",
        "  print(f\"{cls:20s} : {count}\")\n",
        "print(f\"\\nTotal semua gambar: {total_images}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTxeFVJLisOX",
        "outputId": "fac2e783-1d72-4efe-b9c9-9320b62f6467"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jumlah data per kelas (dataset awal):\n",
            "A 801 T              : 108\n",
            "BL                   : 105\n",
            "BM                   : 112\n",
            "CLS 02               : 101\n",
            "JF 03 TA             : 109\n",
            "JF 08 ST             : 173\n",
            "K 406 CTG            : 141\n",
            "K 407 MH             : 201\n",
            "K 409 GWC            : 107\n",
            "V 688 CA             : 227\n",
            "V 697 GKU            : 143\n",
            "V TUL                : 172\n",
            "Y 316 FA             : 123\n",
            "Y 321 C              : 102\n",
            "Y 327 GKU            : 122\n",
            "\n",
            "Total semua gambar: 2046\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install pillow-avif-plugin\n",
        "\n",
        "# --- Imports ---\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import pillow_avif\n",
        "\n",
        "# --- Main Parameters ---\n",
        "ORIG_DIR = '/content/drive/MyDrive/Dataset Skripsi (15 Kelas Kran)'\n",
        "SPLIT_DIR = '/content/drive/MyDrive/Dataset_Skripsi_Split_Kran_15' # Output directory\n",
        "TRAIN_RATIO, VAL_RATIO, TEST_RATIO = 0.70, 0.15, 0.15\n",
        "ALLOWED_EXT = ('.png', '.jpg', '.jpeg', '.webp', '.avif', '.jfif')\n",
        "\n",
        "# --- Utility: Save as JPG (RGB) ---\n",
        "def save_as_jpg(src_path, dst_path, quality=95):\n",
        "    \"\"\"Opens an image, converts it to RGB, and saves it as a JPG.\"\"\"\n",
        "    try:\n",
        "        with Image.open(src_path) as im:\n",
        "            im = im.convert('RGB')\n",
        "            # im = im.resize((224, 224), Image.BICUBIC)\n",
        "            im.save(dst_path, 'JPEG', quality=quality, optimize=True, subsampling=0)\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping {src_path} due to error: {e}\")\n",
        "\n",
        "# --- 1) Splitting and Standardizing to JPG for train/val/test ---\n",
        "print(\"Starting dataset splitting and standardization...\")\n",
        "random.seed(42)  # for reproducible splits\n",
        "\n",
        "# Clean up existing split directory to ensure a fresh start\n",
        "if os.path.exists(SPLIT_DIR):\n",
        "    shutil.rmtree(SPLIT_DIR)\n",
        "    print(f\"Removed existing directory: {SPLIT_DIR}\")\n",
        "\n",
        "# Create the main directory structure\n",
        "os.makedirs(SPLIT_DIR, exist_ok=True)\n",
        "for split in ['train', 'val', 'test']:\n",
        "    os.makedirs(os.path.join(SPLIT_DIR, split), exist_ok=True)\n",
        "\n",
        "class_names = sorted([d for d in os.listdir(ORIG_DIR) if os.path.isdir(os.path.join(ORIG_DIR, d))])\n",
        "print(f\"Found {len(class_names)} classes: {class_names}\")\n",
        "\n",
        "for class_name in class_names:\n",
        "    class_src = os.path.join(ORIG_DIR, class_name)\n",
        "\n",
        "    # Create class subfolders in each split\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        os.makedirs(os.path.join(SPLIT_DIR, split, class_name), exist_ok=True)\n",
        "\n",
        "    # Get all valid image files\n",
        "    images = [p for p in glob(os.path.join(class_src, '*')) if os.path.splitext(p)[1].lower() in ALLOWED_EXT]\n",
        "    random.shuffle(images)\n",
        "\n",
        "    # Calculate split points\n",
        "    total = len(images)\n",
        "    train_end = int(total * TRAIN_RATIO)\n",
        "    val_end = train_end + int(total * VAL_RATIO)\n",
        "\n",
        "    split_map = {\n",
        "        'train': images[:train_end],\n",
        "        'val': images[train_end:val_end],\n",
        "        'test': images[val_end:]\n",
        "    }\n",
        "\n",
        "    # Copy and convert images to the corresponding split folder\n",
        "    for split, files in split_map.items():\n",
        "        for idx, src_path in enumerate(tqdm(files, desc=f\"[{class_name}] -> {split}\", leave=False)):\n",
        "            dst_filename = f\"{class_name}_{idx:05d}.jpg\"\n",
        "            dst_path = os.path.join(SPLIT_DIR, split, class_name, dst_filename)\n",
        "            save_as_jpg(src_path, dst_path, quality=95)\n",
        "\n",
        "print(\"\\n✅ Part 1 Complete: Dataset has been split and standardized into JPG format.\")\n",
        "print(f\"Results saved in: {SPLIT_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuboyMRTXQMg",
        "outputId": "e64e9df5-4a72-4ed8-fd06-4e9ae67dd55a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/4.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.4/4.2 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m4.1/4.2 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hStarting dataset splitting and standardization...\n",
            "Found 15 classes: ['A 801 T', 'BL', 'BM', 'CLS 02', 'JF 03 TA', 'JF 08 ST', 'K 406 CTG', 'K 407 MH', 'K 409 GWC', 'V 688 CA', 'V 697 GKU', 'V TUL', 'Y 316 FA', 'Y 321 C', 'Y 327 GKU']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[K 409 GWC] -> val:  81%|████████▏ | 13/16 [00:00<00:00, 17.44it/s]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "                                                                    "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Part 1 Complete: Dataset has been split and standardized into JPG format.\n",
            "Results saved in: /content/drive/MyDrive/Dataset_Skripsi_Split_Kran_15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Imports ---\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import albumentations as A\n",
        "\n",
        "# --- Main Parameters ---\n",
        "SPLIT_DIR   = '/content/drive/MyDrive/Dataset_Skripsi_Split_Kran_15'       # Input\n",
        "AUGMENT_DIR = '/content/drive/MyDrive/Dataset_Skripsi_Augmented_200_Kran_15' # Output\n",
        "TARGET_TRAIN_PER_CLASS = 200 # Target minimum images per class in the train set\n",
        "\n",
        "# --- Copying and Augmenting the Training Set ---\n",
        "print(\"Starting data augmentation process...\")\n",
        "\n",
        "# Clean up existing augmented directory to ensure a fresh start\n",
        "if os.path.exists(AUGMENT_DIR):\n",
        "    shutil.rmtree(AUGMENT_DIR)\n",
        "    print(f\"Removed existing directory: {AUGMENT_DIR}\")\n",
        "\n",
        "# Copy the entire split dataset to the new augmented directory\n",
        "print(f\"Copying files from {SPLIT_DIR} to {AUGMENT_DIR}...\")\n",
        "shutil.copytree(SPLIT_DIR, AUGMENT_DIR)\n",
        "print(\"Copy complete.\")\n",
        "\n",
        "# Augmentation pipeline\n",
        "augment = A.Compose([\n",
        "    A.RandomBrightnessContrast(p=0.5),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.Rotate(limit=360, p=0.5),\n",
        "    # A.RandomScale(scale_limit=0.2, p=0.5),\n",
        "    # A.GaussNoise(p=0.25),\n",
        "    # A.MotionBlur(p=0.25),\n",
        "    # A.Resize(224, 224)\n",
        "])\n",
        "\n",
        "train_root = os.path.join(AUGMENT_DIR, 'train')\n",
        "class_names = sorted([d for d in os.listdir(train_root) if os.path.isdir(os.path.join(train_root, d))])\n",
        "print(f\"Found {len(class_names)} classes in the training set.\")\n",
        "\n",
        "for class_name in class_names:\n",
        "    class_dir = os.path.join(train_root, class_name)\n",
        "\n",
        "    # Get the current list of JPGs in the train folder for this class\n",
        "    current_jpgs = sorted(glob(os.path.join(class_dir, '*.jpg')))\n",
        "    current_count = len(current_jpgs)\n",
        "    needed = max(0, TARGET_TRAIN_PER_CLASS - current_count)\n",
        "\n",
        "    print(f\"[{class_name}]: {current_count} images exist. Target is {TARGET_TRAIN_PER_CLASS}. Needing {needed} more.\")\n",
        "    if needed == 0:\n",
        "        continue\n",
        "\n",
        "    # Augment images until the target is reached\n",
        "    for i in tqdm(range(needed), desc=f\"Augmenting {class_name}\", leave=False):\n",
        "        # Choose a random image from the original set to augment\n",
        "        src_path = random.choice(current_jpgs)\n",
        "\n",
        "        try:\n",
        "            # Open image and convert to numpy array\n",
        "            image = np.array(Image.open(src_path).convert('RGB'))\n",
        "\n",
        "            # Apply augmentation\n",
        "            augmented = augment(image=image)['image']\n",
        "\n",
        "            # Ensure the data type is correct for saving\n",
        "            if augmented.dtype != np.uint8:\n",
        "                augmented = np.clip(augmented, 0, 255).astype(np.uint8)\n",
        "\n",
        "            # Save the new augmented image with a unique name\n",
        "            save_path = os.path.join(class_dir, f\"aug_{i:06d}.jpg\")\n",
        "            Image.fromarray(augmented).save(save_path, 'JPEG', quality=95, optimize=True)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Could not augment {src_path}: {e}\")\n",
        "            continue\n",
        "\n",
        "print(\"\\n✅ Part 2 Complete: Augmentation of the training set is finished.\")\n",
        "print(f\"Final dataset ready for training in: {AUGMENT_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfsVuJ_vXQal",
        "outputId": "1c08b197-909d-4c77-c94e-418be0383852"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting data augmentation process...\n",
            "Copying files from /content/drive/MyDrive/Dataset_Skripsi_Split_Kran_15 to /content/drive/MyDrive/Dataset_Skripsi_Augmented_200_Kran_15_Ver2...\n",
            "Copy complete.\n",
            "Found 15 classes in the training set.\n",
            "[A 801 T]: 75 images exist. Target is 200. Needing 125 more.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BL]: 73 images exist. Target is 200. Needing 127 more.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BM]: 78 images exist. Target is 200. Needing 122 more.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS 02]: 70 images exist. Target is 200. Needing 130 more.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[JF 03 TA]: 76 images exist. Target is 200. Needing 124 more.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[JF 08 ST]: 121 images exist. Target is 200. Needing 79 more.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[K 406 CTG]: 98 images exist. Target is 200. Needing 102 more.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[K 407 MH]: 140 images exist. Target is 200. Needing 60 more.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[K 409 GWC]: 74 images exist. Target is 200. Needing 126 more.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[V 688 CA]: 158 images exist. Target is 200. Needing 42 more.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[V 697 GKU]: 100 images exist. Target is 200. Needing 100 more.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[V TUL]: 120 images exist. Target is 200. Needing 80 more.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Y 316 FA]: 86 images exist. Target is 200. Needing 114 more.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Y 321 C]: 71 images exist. Target is 200. Needing 129 more.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Y 327 GKU]: 85 images exist. Target is 200. Needing 115 more.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                       "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Part 2 Complete: Augmentation of the training set is finished.\n",
            "Final dataset ready for training in: /content/drive/MyDrive/Dataset_Skripsi_Augmented_200_Kran_15_Ver2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    }
  ]
}